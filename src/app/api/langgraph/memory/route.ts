import { PromptTemplate } from "@langchain/core/prompts";
import { PromptTemplateJson } from "@/contents/type";
import { Message as VercelChatMessage, LangChainAdapter } from "ai";
import {
  Annotation,
  MemorySaver,
  MessageGraph,
  messagesStateReducer,
  StateGraph,
  MessagesAnnotation,
  END,
} from "@langchain/langgraph";

import { getModel, isObject, loadJsonFile } from "@/contents/utils";
import {
  BaseMessage,
  HumanMessage,
  RemoveMessage,
  SystemMessage,
  trimMessages,
} from "@langchain/core/messages";

// ãƒãƒ£ãƒƒãƒˆå½¢å¼
const formatMessage = (message: VercelChatMessage) => {
  return `${message.role}: ${message.content}`;
};

/**
 * éå»å±¥æ­´ã‚’åˆ‡ã‚Šæ¨ã¦ã‚‹ã‚‚ã®
 * ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ã§å±¥æ­´å‰Šé™¤: ConversationBufferWindowMemory
 * ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§ã®å±¥æ­´å‰Šé™¤: ConversationTokenBufferMemory
 */
const trimmer = trimMessages({
  maxTokens: 5, // æ®‹ã™æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
  strategy: "last",
  tokenCounter: (messages: BaseMessage[]) => messages.length, // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ã§ã‚«ã‚¦ãƒ³ãƒˆ
  includeSystem: true, // ã‚·ã‚¹ãƒ†ãƒ ã‚’å«ã‚ã‚‹ã‹
  allowPartial: false,
  startOn: "human",
});

/**
 * ã‚°ãƒ©ãƒ•ã‚µãƒ³ãƒ—ãƒ«
 */
/** LLM ã‚’å‘¼ã¶é–¢æ•° */
async function callModel(state: typeof GraphAnnotation.State) {
  console.log("ğŸ§  call model");
  const model = getModel("gpt-4o-mini");

  // ä¼šè©±å±¥æ­´å‰Šé™¤
  // const trimmed = await trimmer.invoke(state.messages);

  // ä¼šè©±å±¥æ­´è¦ç´„
  let messages;
  const summary = state.summary;
  // è¦ç´„ãŒå­˜åœ¨ã™ã‚‹å ´åˆã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦è¿½åŠ 
  if (summary) {
    const systemMessage = `å‰å›ã®ä¼šè©±ã®è¦ç´„: ${summary}`;
    messages = [new SystemMessage(systemMessage), ...state.messages];
  } else {
    messages = state.messages;
  }
  const response = await model.invoke(messages);
  console.log(response.content);

  return { messages: response };
}

/** ä¼šè©±ã‚’è¡Œã†ã‹è¦ç´„ã™ã‚‹ã‹ã®åˆ¤æ–­å‡¦ç† */
async function shouldContenue(state: typeof GraphAnnotation.State) {
  console.log("â“ should contenue");
  const messages = state.messages;

  if (messages.length > 6) return "summarize";
  return END;
}

/** ä¼šè©±ã®è¦ç´„å‡¦ç† */
async function summarizeConversation(state: typeof GraphAnnotation.State) {
  console.log("ğŸ“ƒ summarize conversation");
  const model = getModel("gpt-4o-mini");
  const summary = state.summary;

  let summaryMessage;
  if (summary) {
    summaryMessage = `ã“ã‚Œã¾ã§ã®ä¼šè©±ã®è¦ç´„: ${summary}\n\nä¸Šè¨˜ã®æ–°ã—ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è€ƒæ…®ã—ã¦è¦ç´„ã‚’æ‹¡å¼µã—ã¦ãã ã•ã„ã€‚: `;
  } else {
    summaryMessage =
      "ä¸Šè¨˜ã®ä¼šè©±ã®è¦ç´„ã‚’ä¼šè©±ã®æµã‚Œã‚’é‡è¦–ã—ã¦ç®‡æ¡æ›¸ãã§ä½œæˆã—ã¦ãã ã•ã„: ";
  }

  // è¦ç´„å‡¦ç†
  const messages = [...state.messages, new SystemMessage(summaryMessage)];
  const response = await model.invoke(messages);
  console.log(response.content);

  const deleteMessages = messages
    .slice(0, -2)
    .map((m) => new RemoveMessage({ id: m.id! }));
  return { summary: response.content, messages: deleteMessages };
}

// ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®è¿½åŠ 
const GraphAnnotation = Annotation.Root({
  summary: Annotation<string>(),
  ...MessagesAnnotation.spec,
});

// ã‚°ãƒ©ãƒ•
const workflow = new StateGraph(GraphAnnotation)
  // ãƒãƒ¼ãƒ‰è¿½åŠ 
  .addNode("conversation", callModel)
  .addNode("summarize", summarizeConversation)

  // ã‚¨ãƒƒã‚¸è¿½åŠ 
  .addEdge("__start__", "conversation")
  .addConditionalEdges("conversation", shouldContenue)
  .addEdge("summarize", END);

// è¨˜æ†¶ã®è¿½åŠ 
const memory = new MemorySaver();
const app = workflow.compile({ checkpointer: memory });

/**
 * ãƒãƒ£ãƒƒãƒˆå¿œç­”AIï¼ˆè¨˜æ†¶ãƒ»ãƒ¢ãƒ‡ãƒ«å¤‰æ›´å¯¾å¿œæ¸ˆã¿ï¼‰
 * @param req
 * @returns
 */
export async function POST(req: Request) {
  try {
    const body = await req.json();
    const messages = body.messages ?? [];
    const modelName = body.model ?? "fake-llm";

    // éå»ã®å±¥æ­´{chat_history}
    const formattedPreviousMessages = messages.slice(0, -1).map(formatMessage);

    // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸{input}
    const currentMessageContent = messages[messages.length - 1].content;

    //ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ä½œæˆ
    const template = await loadJsonFile<PromptTemplateJson[]>(
      "src/data/prompt-template.json"
    );
    if (!template.success) {
      return new Response(JSON.stringify({ error: template.error }), {
        status: 500,
        headers: { "Content-type": "application/json" },
      });
    }
    // ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®æŠ½å‡º
    const found = template.data.find(
      (obj) => isObject(obj) && obj["name"] === "api-chat-aikato"
    );
    if (!found) {
      throw new Error("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ");
    }

    /**
     * ConversationBufferMemoryã«ã‚ãŸã‚‹ã‚‚ã®
     * å±¥æ­´å…¨éƒ¨ä¿æœ‰
     */
    const config = { configurable: { thread_id: "abc123" } }; // ID ã§ä¼šè©±å±¥æ­´ã‚’å‚ç…§

    // æ—¢å­˜ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å–å¾—
    const state = await app.getState(config);
    const existingMessages = state?.values?.messages || [];
    // console.log(existingMessages);

    // å±¥æ­´ã‚’å«ã¾ã›ã‚‹
    const graphResult = await app.invoke(
      {
        messages: currentMessageContent,
      },
      config
    );

    // console.log("langgraph: " + graphResult.messages[graphResult.messages.length - 1].content);

    // ãƒ¢ãƒ‡ãƒ«ã®æŒ‡å®š
    const model = getModel(modelName);

    const prompt = PromptTemplate.fromTemplate(found.template);
    const chain = prompt.pipe(model);

    const stream = await chain.stream({
      history: formattedPreviousMessages.join("\n"),
      input: currentMessageContent,
    });

    return LangChainAdapter.toDataStreamResponse(stream);
  } catch (error) {
    if (error instanceof Error) {
      return new Response(JSON.stringify({ error: error.message }), {
        status: 500,
        headers: { "Content-Type": "application/json" },
      });
    }

    return new Response(JSON.stringify({ error: "Unknown error occurred" }), {
      status: 500,
      headers: { "Content-Type": "application/json" },
    });
  }
}
